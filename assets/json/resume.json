{"basics":{"name":"Seyed Hossein Ghafarian","label":"Machine Learning Researcher","image":"","email":"s.h.ghafarian@gmail.com","phone":"123-4567","url":"https://hosseinghafarian.github.io/","summary":"My work explores a central question: how can machines learn robustly from so little, whether it's a handful of examples or the underlying structure of the world itself.","location":{"address":"Quchan University of Technology","postalCode":"CA 94115","city":"Mashhad","countryCode":"Ir","region":"Khorasan"},"profiles":[{"network":"LinkedIn","username":"AlbertEinstein","url":"https://twitter.com/AlbertEinstein"}]},"work":[{"name":"Quchan University of Technology","position":"Assistant Professor of Computer Science","url":"https://hosseinghafarian.github.io/","startDate":"2007-10-01","endDate":"2025-10-01","summary":"Teaching computer science and related mathematics courses.","highlights":["Machine Learning"]}],"education":[{"institution":"Ferdowsi University of Mashhad","location":"Mashhad, Iran","url":"https://www.um.ac.ir/","area":"Artificial Intelligence","studyType":"PhD","startDate":"2012-01-01","endDate":"2019-07-01","score":"20","courses":["Active learning in Input and Parameter Spaces"]}],"publications":[{"name":"Prepare for the Worst, Hope for the Best: Active Robust Learning On Distributions","publisher":"IEEE Transactions on Cybernetics","releaseDate":"2021-06-30","url":"https://ieeexplore.ieee.org/document/9440861","summary":"In recent years, many learning systems have been developed for higher level forms of data, such as learning on distributions in which each example itself is a distribution. This article proposes active robust learning on distributions. In learning on distributions, there is no access to distributions themselves but rather access is through a sample drawn from a distribution. Therefore, similar to robust learning, any estimates of examples are inexact. In order to address these difficulties, we provide an upper bound on the risk of the classifier in the next stage of active learning, where the size of the labeled dataset increases. Based on this upper bound, we propose probabilistic minimax active learning (PMAL) as a general multiclass active learning method that is easy to use in many Bayesian settings, which provably selects an example with knowledge of its label minimizing the expected risk. We present an efficient approximation of the objective with a known error bound to deal with the intractability of the proposed method for active robust learning. Here, we face a nonconvex problem, which we solve by means of a related convex problem with a bound on the norm of the difference between their solutions. To utilize the information about the estimates of distributions, we propose active robust learning on the distributions method based on learning the kernel embedding of distributions by a recent Bayesian method. The experiments demonstrate the effectiveness of the resulting method on a set of synthetic and real-world distributional datasets."},{"name":"Identifying crisis-related informative tweets using learning on distributions","publisher":"Information Processing and Management","releaseDate":"2020-03-01","url":"https://www.sciencedirect.com/science/article/abs/pii/S030645731930322X","summary":"Learning on distributions achieves very good results in identifying informative tweets about a crisis incident."},{"name":"Functional gradient approach to probabilistic minimax active learning","publisher":"Engineering Applications of Artificial Intelligenc","releaseDate":"2019-10-20","url":"https://www.sciencedirect.com/science/article/abs/pii/S0952197619301228","summary":"Many active learning methods select informative and representative examples by employing a parametric approach. However, there is very limited research pursuing a functional non-parametric approach to informative and representative active learning. The present study proposes a general functional approach to active learning based on a minimax objective function. Using this general algorithm, the current paper presents a specific algorithm based on a simple class of functions. Experiments show that the proposed method is efficient in selecting examples. It is interesting that the resulting algorithm can be interpreted from a spectral filtering perspective. This establishes a relationship between active learning, boosting, and spectral filtering and opens up new avenues for developing even better active learning algorithms."},{"name":"Local variational Probabilistic Minimax Active Learning","publisher":"Expert Systems with Applications","releaseDate":"2023-01-10","url":"https://www.sciencedirect.com/science/article/abs/pii/S0957417422016128","summary":"Many active learning methods select informative and representative examples by employing a parametric approach. However, there is very limited research pursuing a functional non-parametric approach to informative and representative active learning. The present study proposes a general functional approach to active learning based on a minimax objective function. Using this general algorithm, the current paper presents a specific algorithm based on a simple class of functions. Experiments show that the proposed method is efficient in selecting examples. It is interesting that the resulting algorithm can be interpreted from a spectral filtering perspective. This establishes a relationship between active learning, boosting, and spectral filtering and opens up new avenues for developing even better active learning algorithms."}],"skills":[{"name":"Python, C++, Matlab","level":"Master","icon":"fa-solid fa-hashtag","keywords":["Scientific computing","Numpy","Pandas","Scipy"]},{"name":"Pytorch","level":"Master","icon":"fa-solid fa-hashtag","keywords":["Building deep learning models"]},{"name":"Pytorch Geometric","level":"Master","icon":"fa-solid fa-hashtag","keywords":["Building Graph Neural Network models"]},{"name":"Software Engineering for ML","level":"Master","icon":"fa-solid fa-hashtag","keywords":["Developing production-grade ML code with an emphasis on modularity and maintainability.","Applying design principles (SOLID, Clean Code) to build scalable and robust systems.","System architecture focused on separation of concerns between data, training, and serving pipelines."]}],"languages":[{"language":"Persian","fluency":"Native speaker","icon":""},{"language":"English","fluency":"Fluent","icon":""}],"interests":[{"name":"Mashine Learning","icon":"fa-solid fa-tag","keywords":["Active Learning","Active Inference","Graph Neural Networks","Deep Learning","Bayesian Inference"]}],"references":[{"name":"Professor John Doe","icon":"fa-solid fa-laptop","reference":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam condimentum, diam quis convallis euismod, arcu mi ullamcorper lorem, a vestibulum nunc magna at sem. Sed in risus ac felis varius blandit. D"},{"name":"Professor John Doe","icon":"fa-solid fa-thumbtack","reference":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam condimentum, diam quis convallis euismod, arcu mi ullamcorper lorem, a vestibulum nunc magna at sem. Sed in risus ac felis varius blandit. D"}],"projects":[{"name":"Quantum Computing","summary":"Quantum computing is the use of quantum-mechanical phenomena such as superposition and entanglement to perform computation. Computers that perform quantum computations are known as quantum computers.","highlights":["Quantum Teleportation","Quantum Cryptography"],"startDate":"2018-01-01","endDate":"2018-01-01","url":"https://example.com"}]}