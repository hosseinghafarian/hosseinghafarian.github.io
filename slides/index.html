<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Slides | Seyed Hossein Ghafarian </title> <meta name="author" content="Seyed Hossein Ghafarian"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?1fefb6021bf36010f25ea1cef24af84e"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?53a094b51ed1d1e025731eb00d240058" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hosseinghafarian.github.io/slides/"> <script src="/assets/js/theme.js?f2531f05c6f8e1622518f4f5a1e385b1"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?46af317e693b09921dcb92261d123fbc" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Seyed</span> Hossein Ghafarian </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item active"> <a class="nav-link" href="/slides/">Slides <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Slides</h1> <p class="post-description"></p> </header> <article> <h1>Slides</h1> <ul class="slides-list" style="list-style:none;padding-left:0;"> <li style="margin-bottom:1.2rem;"> <h2 style="margin:0; font-size:1.05rem;"> <a href="/slides/sltferdowsi/">Statistical Learning Theory</a> </h2> <p>The Intellectual Architecture of Learning: From <code class="language-plaintext highlighter-rouge">Empirical Risk</code> to the <code class="language-plaintext highlighter-rouge">Falsifiability</code> of Intelligence This presentation outlines a journey through the theoretical foundations that underpin modern machine learning, moving beyond mere algorithmic recipes to the profound philosophical and mathematical principles that make learning from data possible.</p> <p>It begins with the fundamental, deceptively simple goal: to learn a function from empirical data. We are immediately introduced to the core paradigm of Risk Minimization—the quest to minimize the expected loss. The central tragedy of learning is laid bare: this true risk is a ghost in the machine, an integral over an unknown distribution P ( x , y ) P(x,y), forever inaccessible to us. Our only tangible evidence is the finite, and often meager, training set.</p> <p>This dilemma forces us onto the path of <code class="language-plaintext highlighter-rouge">Empirical Risk Minimization (ERM)</code>, where we minimize the loss on the observed data. But this is a Faustian bargain. The presentation masterfully illustrates the <code class="language-plaintext highlighter-rouge">Bias-Variance Dilemma</code> not just as a trade-off between underfitting and overfitting, but as a fundamental choice about the complexity of our reality. Do we, like physicists, impose a simple model and attribute error to measurement, or do we let the data dictate a complex, potentially spurious, narrative?</p> <p>The narrative then takes a decisive turn, answering a critical question: why can’t we simply use the set of all possible functions to achieve zero empirical error? The answer is the cornerstone of Statistical Learning Theory (SLT): <code class="language-plaintext highlighter-rouge">consistency</code>. Through a compelling counterexample, we see that a memorizing function that achieves zero training error learns nothing; it is a vacuous exercise in pattern storage without generalization. This leads to the pivotal Key Theorem: ERM is consistent if and only if the empirical risk converges uniformly to the actual risk over the entire function class.</p> <p>This requirement for uniform convergence forces us to confront the capacity of our learning machines. The presentation elegantly introduces the tools to measure this capacity: the growth of the number of distinct dichotomies, or <code class="language-plaintext highlighter-rouge">shatterings</code>, a set of functions can achieve on a sample. This is formalized through the concepts of Entropy, Annealed VC-Entropy, and the Growth Function.</p> <p>Here, the presentation reveals its deep philosophical roots, connecting learning theory to <code class="language-plaintext highlighter-rouge">Karl Popper's Demarcation Problem</code>. It draws a brilliant parallel: <code class="language-plaintext highlighter-rouge">a theory that can explain everything (e.g., astrology) explains nothing</code>. Similarly, a nonfalsifiable learning machine—one with a growth function that is linear, meaning it can shatter any set of points—is useless. It can fit any dataset perfectly but possesses no power of generalization. True learning, like true science, requires a falsifiable model; it must be capable of being wrong. This is formally captured by the condition that the VC entropy must grow sub-linearly with the sample size.</p> <p>The climax of the theoretical exposition is the introduction of the VC Dimension, a single number that captures the intrinsic capacity of a model. The slides present the remarkable, almost magical, property of the Growth Function: it is either linear or bounded by a logarithmic function. This binary nature leads to the celebrated generalization bounds, which provide non-asymptotic, probabilistic guarantees on the performance of a model, showing that the true risk is bounded by the empirical risk plus a term dependent on the VC dimension.</p> <p>Armed with this theory, the presentation transitions from “what is possible” to “how to build.” It introduces the <code class="language-plaintext highlighter-rouge">Structural Risk Minimization (SRM)</code> principle as the constructive alternative to the naive ERM. SRM provides a structured way to navigate the bias-variance trade-off by simultaneously minimizing empirical risk and the capacity term from the generalization bound, effectively implementing a principled form of Occam’s Razor. It is noted that Vapnik’s framework refutes the naive interpretation of Occam’s Razor; it is not about the “simplest” model in terms of parameters, but the model with the smallest capacity (VC-dimension) that fits the data.</p> <p>The presentation concludes by linking these theoretical pillars to practical algorithms, positioning Support Vector Machines (SVMs) as a direct embodiment of these principles. By constructing a maximal margin hyperplane, SVMs actively control the VC dimension (which is inversely proportional to the margin), thus providing a concrete mechanism for SRM. A final, forward-looking note on Rademacher Complexity hints at the evolution of these ideas, offering potentially sharper, distribution-dependent bounds.</p> <p>In summary, this presentation is not merely a collection of theorems. It is a coherent narrative that weaves together probability, statistics, and philosophy to answer the most profound question in machine learning: How can finite evidence justify infinite conclusions? It demonstrates that <code class="language-plaintext highlighter-rouge">generalization is not a happy accident but a mathematical consequence of using a falsifiable model with controlled capacity</code>, a principle that remains as relevant to today’s deep learning systems as it was to the linear classifiers of the past.</p> <p style="margin:.2rem 0 .4rem; color:#6c757d; font-size:.95rem;"> February 10, 2012 — Ferdowsi University of Mashhad, Iran, with supervision of Prof Sadoghi Yazdi </p> <p style="margin:.2rem 0;"> <a href="/assets/slides/SLTinferdowsi.pdf" rel="noopener" target="_blank">Download slides</a> — An short introduction to Statistical Learning. </p> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Seyed Hossein Ghafarian. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?c999e8c5281874b7534e3352f835d4c3" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?b977fe0c21b2118ed853308b1b923969"></script> <script src="/assets/js/no_defer.js?699fa7cbe3b29f831db7d5250ba3203a"></script> <script defer src="/assets/js/common.js?d3a25b46bbd2e0a751a27b173abc6e5f"></script> <script defer src="/assets/js/copy_code.js?fff63901a03063094790ffbfd4bc0cb4" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?25eff8ff4144a010e4ad7b31403102cf"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?04761245f225e3ad9e5e3f875d4e1074"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?b78cb42895d74e4fd6de6f633edcf181" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?230637657ac0b42e92d76e2b4c1b4764"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?ade73d6d60912d119f9c9347bc176630"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?f74bfa9a88ab862fb9df1c46146b7b7d"></script> </body> </html>